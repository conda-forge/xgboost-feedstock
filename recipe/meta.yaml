{% set name = "xgboost" %}
{% set version = "0.80" %}
{% set sha256 = "ec990238943988e73734ad483cf6c53b81aec9745811c7b6c5cbb189ef5e328b" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  fn: {{ name }}-{{ version }}.tar.gz
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: {{ sha256 }}
  patches:
    - config.mk.patch  # [osx]

build:
  number: 0
  skip: True  # [win]
  script: python setup.py install --single-version-externally-managed --record record.txt

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
  host:
    - python
    - setuptools
    - clangdev
    - libcxx 4.0.*  # [osx]
    - openmp 4.0.*  # [osx]

  run:
    - python
    - numpy
    - scipy
    - libcxx >=4.0  # [osx]
    - openmp >=4.0  # [osx]

test:
  requires:
    - scikit-learn >=0.18.0
    - numpy
    - scipy

  imports:
    - xgboost

about:
  home: https://github.com/dmlc/xgboost
  license: Apache-2.0
  license_file: {{ RECIPE_DIR }}/LICENSE
  summary: |
    Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library, for
    Python, R, Java, Scala, C++ and more. Runs on single machine, Hadoop, Spark, Flink
    and DataFlow
  description: |
    XGBoost is an optimized distributed gradient boosting library designed to be highly efficient,
    flexible and portable. It implements machine learning algorithms under the Gradient Boosting
    framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many
    data science problems in a fast and accurate way. The same code runs on major distributed
    environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.
  doc_url: https://xgboost.readthedocs.io/
  dev_url: https://github.com/dmlc/xgboost/

extra:
  recipe-maintainers:
    - beckermr
    - aldanor
